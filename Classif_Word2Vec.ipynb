{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 2.7.15+ (default, Nov 27 2018, 23:36:35) \n",
      "[GCC 7.3.0]\n",
      "NLTK: 3.2.5\n",
      "Scikit-learn: 0.20.3\n",
      "Pandas: 0.24.2\n",
      "Numpy: 1.14.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import sklearn\n",
    "import pandas\n",
    "import numpy\n",
    "\n",
    "print('Python: {}'.format(sys.version))\n",
    "print('NLTK: {}'.format(nltk.__version__))\n",
    "print('Scikit-learn: {}'.format(sklearn.__version__))\n",
    "print('Pandas: {}'.format(pandas.__version__))\n",
    "print('Numpy: {}'.format(numpy.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 793 entries, 0 to 792\n",
      "Data columns (total 2 columns):\n",
      "0    793 non-null bool\n",
      "1    793 non-null object\n",
      "dtypes: bool(1), object(1)\n",
      "memory usage: 7.0+ KB\n",
      "None\n",
      "       0                                                  1\n",
      "0  False  inexecuca contrat sanca afast reexam prov clau...\n",
      "1  False  acident aer decretol n dol eventual respons at...\n",
      "2  False  intimaca art cpc irregular nao comprov recurs ...\n",
      "3  False  honorari execuca legitim part superior tribuna...\n",
      "4  False  servidor public artig lei n artig lei n revoga...\n"
     ]
    }
   ],
   "source": [
    "# Carregando dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('data/dataset_tratado.txt', header=None, encoding='utf-8', sep='\\t')\n",
    "print(df.info())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    525\n",
      "True     268\n",
      "Name: 0, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# verificação das classes dos acórdãos\n",
    "\n",
    "binary = df[0]\n",
    "print(binary.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 1 1 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "793"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# convertendo classes\n",
    "encoder = LabelEncoder()\n",
    "binary = encoder.fit_transform(binary)\n",
    "\n",
    "print(binary[:10])\n",
    "len(binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    inexecuca contrat sanca afast reexam prov clau...\n",
      "1    acident aer decretol n dol eventual respons at...\n",
      "2    intimaca art cpc irregular nao comprov recurs ...\n",
      "3    honorari execuca legitim part superior tribuna...\n",
      "4    servidor public artig lei n artig lei n revoga...\n",
      "5    iss arrend mercantil incidenc mat constitucion...\n",
      "6    processual penal art codig process penal conhe...\n",
      "7    recurs especial art incis art ambos codig pena...\n",
      "8    praz prescricional lei complement n artig lei ...\n",
      "9    licitaca aca anulator indefer tutel antecip ag...\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# criação da lista de ementas\n",
    "ementas = df[1]\n",
    "print(ementas[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=3410, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# uso de word2vec para a extração de features\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "ementas_list = [word for word in ementas.iteritems()]\n",
    "ementas_list_list = []\n",
    "for doc in ementas_list:\n",
    "    ementas_list_list.append(doc[1].split())\n",
    "\n",
    "X = Word2Vec(ementas_list_list, min_count=2)\n",
    "X.wv.init_sims() # para inicializar model.vw.syn0norm, necessário para o cálculo da média da\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funções para calcular a média vetorial das palavras de cada ementa\n",
    "\n",
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "            mean.append(wv.vectors[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "        # FIXME: remove these examples in pre-processing\n",
    "        return np.zeros(wv.vector_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, post) for post in text_list ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# função para tokenizar as palavras com frequência maior que 2\n",
    "\n",
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.14534767  0.01982272  0.0448398  ... -0.07477809  0.02976305\n",
      "  -0.04991043]\n",
      " [ 0.12520508  0.02962115  0.05239616 ... -0.06825581  0.03478844\n",
      "  -0.02642938]\n",
      " [ 0.1306565   0.04340264  0.04938867 ... -0.07956544  0.02947289\n",
      "  -0.02057191]\n",
      " ...\n",
      " [ 0.13771681  0.02761975  0.0499327  ... -0.07228079  0.030251\n",
      "  -0.0448534 ]\n",
      " [ 0.1387119   0.04569263  0.04449716 ... -0.07536625  0.03929668\n",
      "  -0.03407298]\n",
      " [ 0.13451058  0.02732372  0.05012171 ... -0.07068528  0.03359158\n",
      "  -0.03754881]]\n",
      "[[ 0.13530824  0.04177387  0.04626328 ... -0.07259557  0.03818455\n",
      "  -0.03387338]\n",
      " [ 0.17629535 -0.0101957   0.03212216 ... -0.08912148  0.02519407\n",
      "  -0.06986885]\n",
      " [ 0.1120663   0.04584334  0.05504971 ... -0.0656649   0.03239399\n",
      "  -0.01685729]\n",
      " ...\n",
      " [ 0.12619235  0.03421398  0.04937165 ... -0.07320057  0.03131828\n",
      "  -0.02328027]\n",
      " [ 0.12410226  0.03746981  0.05848242 ... -0.05842689  0.04324841\n",
      "  -0.03667566]\n",
      " [ 0.16727343 -0.00660453  0.03137209 ... -0.09150209  0.02583397\n",
      "  -0.04554113]]\n"
     ]
    }
   ],
   "source": [
    "# Like any other supervised machine learning problem, we need to divide data into \n",
    "# 20% test set and 80% training set\n",
    "\n",
    "import gensim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=0)\n",
    "\n",
    "test_tokenized = test.apply(lambda r: w2v_tokenize_text(r[1]), axis=1).values\n",
    "train_tokenized = train.apply(lambda r: w2v_tokenize_text(r[1]), axis=1).values\n",
    "\n",
    "X_train = word_averaging_list(X.wv,train_tokenized)\n",
    "X_test = word_averaging_list(X.wv,test_tokenized)\n",
    "\n",
    "y_train = train[0]\n",
    "y_test = test[0]\n",
    "\n",
    "print(X_train)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Nearest Neighbors Accuracy: 61.6352201258\n",
      "Decision Tree Accuracy: 70.4402515723\n",
      "Random Forest Accuracy: 71.0691823899\n",
      "Logistic Regression Accuracy: 62.893081761\n",
      "SGD Classifier Accuracy: 62.893081761\n",
      "Naive Bayes Accuracy: 62.893081761\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "\n",
    "# Modelos para treinar\n",
    "names = [\"K Nearest Neighbors\", \"Decision Tree\", \"Random Forest\", \"Logistic Regression\", \"SGD Classifier\",\n",
    "         \"Naive Bayes\", \"SVM Linear\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(n_estimators=1000, random_state=0),\n",
    "    LogisticRegression(solver='lbfgs'),\n",
    "    SGDClassifier(max_iter = 100),\n",
    "    SVC(kernel = 'linear')\n",
    "]\n",
    "\n",
    "models = zip(names, classifiers)\n",
    "\n",
    "for name, classifier in models:\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)*100\n",
    "    print(\"{} Accuracy: {}\".format(name, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier: Accuracy: 66.6666666667\n"
     ]
    }
   ],
   "source": [
    "# Classificador por votos\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "names = [\"K Nearest Neighbors\", \"Decision Tree\", \"Random Forest\", \"Logistic Regression\", \"SGD Classifier\",\n",
    "         \"Naive Bayes\", \"SVM Linear\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(n_estimators=1000, random_state=0),\n",
    "    LogisticRegression(solver='lbfgs'),\n",
    "    SGDClassifier(max_iter = 100),\n",
    "    SVC(kernel = 'linear')\n",
    "]\n",
    "\n",
    "models = list(zip(names, classifiers))\n",
    "zipped_models_1 = models[:]\n",
    "zipped_models_2 = list(models)\n",
    "\n",
    "\n",
    "votingClassifier = VotingClassifier(estimators = zipped_models_2, voting = 'hard', n_jobs = -1)\n",
    "votingClassifier.fit(X_train, y_train)\n",
    "y_pred = votingClassifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)*100\n",
    "print(\"Voting Classifier: Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[93  7]\n",
      " [46 13]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.67      0.93      0.78       100\n",
      "        True       0.65      0.22      0.33        59\n",
      "\n",
      "   micro avg       0.67      0.67      0.67       159\n",
      "   macro avg       0.66      0.58      0.55       159\n",
      "weighted avg       0.66      0.67      0.61       159\n",
      "\n",
      "66.66666666666666\n"
     ]
    }
   ],
   "source": [
    "# relatório de classificação\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print((accuracy_score(y_test, y_pred)*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
