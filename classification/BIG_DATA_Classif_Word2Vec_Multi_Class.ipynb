{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Python: 3.6.3 (default, Oct  3 2017, 21:45:48) \n[GCC 7.2.0]\nNLTK: 3.4.4\nScikit-learn: 0.21.3\nPandas: 0.25.0\nNumpy: 1.17.0\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import sklearn\n",
    "import pandas\n",
    "import numpy\n",
    "\n",
    "print('Python: {}'.format(sys.version))\n",
    "print('NLTK: {}'.format(nltk.__version__))\n",
    "print('Scikit-learn: {}'.format(sklearn.__version__))\n",
    "print('Pandas: {}'.format(pandas.__version__))\n",
    "print('Numpy: {}'.format(numpy.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1653762 entries, 0 to 1653761\nData columns (total 2 columns):\n0    1653762 non-null object\n1    1653762 non-null object\ndtypes: object(2)\nmemory usage: 25.2+ MB\nNone\n            0                                                  1\n0      outros  pretensa estadual deduz apel extrem esbarr ved...\n1       civil  embarg insist mesm razo recurs apresent oposic...\n2  processual  acorda ora recorr nao neg vigenc direit federa...\n3      outros  segund jurisprudenc superior tribunal justic a...\n4      outros  especial superior tribunal justic decid ser ne...\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Carregando dados\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/big/dataset_multi.txt', header=None, encoding='utf-8', sep='\\t')\n",
    "df.fillna('x', inplace=True)\n",
    "#df_classes_raw = pd.read_csv('data/classes_raw.txt', header=None, encoding='utf-8', sep='\\t')\n",
    "print(df.info())\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "10\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<pandas.io.formats.style.Styler at 0x7f09f8714588>",
      "text/html": "<style  type=\"text/css\" >\n    #T_3ddac544_bf74_11e9_8188_d05099a0e2b9row0_col0 {\n            width:  10em;\n             height:  80%;\n            background:  linear-gradient(90deg,#d65f5f 100.0%, transparent 100.0%);\n        }    #T_3ddac544_bf74_11e9_8188_d05099a0e2b9row1_col0 {\n            width:  10em;\n             height:  80%;\n            background:  linear-gradient(90deg,#d65f5f 3.7%, transparent 3.7%);\n        }    #T_3ddac544_bf74_11e9_8188_d05099a0e2b9row2_col0 {\n            width:  10em;\n             height:  80%;\n            background:  linear-gradient(90deg,#d65f5f 3.7%, transparent 3.7%);\n        }    #T_3ddac544_bf74_11e9_8188_d05099a0e2b9row3_col0 {\n            width:  10em;\n             height:  80%;\n            background:  linear-gradient(90deg,#d65f5f 3.6%, transparent 3.6%);\n        }    #T_3ddac544_bf74_11e9_8188_d05099a0e2b9row4_col0 {\n            width:  10em;\n             height:  80%;\n            background:  linear-gradient(90deg,#d65f5f 3.3%, transparent 3.3%);\n        }    #T_3ddac544_bf74_11e9_8188_d05099a0e2b9row5_col0 {\n            width:  10em;\n             height:  80%;\n            background:  linear-gradient(90deg,#d65f5f 2.5%, transparent 2.5%);\n        }    #T_3ddac544_bf74_11e9_8188_d05099a0e2b9row6_col0 {\n            width:  10em;\n             height:  80%;\n            background:  linear-gradient(90deg,#d65f5f 1.8%, transparent 1.8%);\n        }    #T_3ddac544_bf74_11e9_8188_d05099a0e2b9row7_col0 {\n            width:  10em;\n             height:  80%;\n            background:  linear-gradient(90deg,#d65f5f 1.4%, transparent 1.4%);\n        }    #T_3ddac544_bf74_11e9_8188_d05099a0e2b9row8_col0 {\n            width:  10em;\n             height:  80%;\n            background:  linear-gradient(90deg,#d65f5f 0.0%, transparent 0.0%);\n        }    #T_3ddac544_bf74_11e9_8188_d05099a0e2b9row9_col0 {\n            width:  10em;\n             height:  80%;\n        }</style><table id=\"T_3ddac544_bf74_11e9_8188_d05099a0e2b9\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >0</th>    </tr></thead><tbody>\n                <tr>\n                        <th id=\"T_3ddac544_bf74_11e9_8188_d05099a0e2b9level0_row0\" class=\"row_heading level0 row0\" >outros</th>\n                        <td id=\"T_3ddac544_bf74_11e9_8188_d05099a0e2b9row0_col0\" class=\"data row0 col0\" >1372026</td>\n            </tr>\n            <tr>\n                        <th id=\"T_3ddac544_bf74_11e9_8188_d05099a0e2b9level0_row1\" class=\"row_heading level0 row1\" >previdenciário</th>\n                        <td id=\"T_3ddac544_bf74_11e9_8188_d05099a0e2b9row1_col0\" class=\"data row1 col0\" >51518</td>\n            </tr>\n            <tr>\n                        <th id=\"T_3ddac544_bf74_11e9_8188_d05099a0e2b9level0_row2\" class=\"row_heading level0 row2\" >constitucional</th>\n                        <td id=\"T_3ddac544_bf74_11e9_8188_d05099a0e2b9row2_col0\" class=\"data row2 col0\" >51225</td>\n            </tr>\n            <tr>\n                        <th id=\"T_3ddac544_bf74_11e9_8188_d05099a0e2b9level0_row3\" class=\"row_heading level0 row3\" >tributário</th>\n                        <td id=\"T_3ddac544_bf74_11e9_8188_d05099a0e2b9row3_col0\" class=\"data row3 col0\" >50194</td>\n            </tr>\n            <tr>\n                        <th id=\"T_3ddac544_bf74_11e9_8188_d05099a0e2b9level0_row4\" class=\"row_heading level0 row4\" >processual</th>\n                        <td id=\"T_3ddac544_bf74_11e9_8188_d05099a0e2b9row4_col0\" class=\"data row4 col0\" >46082</td>\n            </tr>\n            <tr>\n                        <th id=\"T_3ddac544_bf74_11e9_8188_d05099a0e2b9level0_row5\" class=\"row_heading level0 row5\" >administrativo</th>\n                        <td id=\"T_3ddac544_bf74_11e9_8188_d05099a0e2b9row5_col0\" class=\"data row5 col0\" >34629</td>\n            </tr>\n            <tr>\n                        <th id=\"T_3ddac544_bf74_11e9_8188_d05099a0e2b9level0_row6\" class=\"row_heading level0 row6\" >penal</th>\n                        <td id=\"T_3ddac544_bf74_11e9_8188_d05099a0e2b9row6_col0\" class=\"data row6 col0\" >26102</td>\n            </tr>\n            <tr>\n                        <th id=\"T_3ddac544_bf74_11e9_8188_d05099a0e2b9level0_row7\" class=\"row_heading level0 row7\" >civil</th>\n                        <td id=\"T_3ddac544_bf74_11e9_8188_d05099a0e2b9row7_col0\" class=\"data row7 col0\" >19844</td>\n            </tr>\n            <tr>\n                        <th id=\"T_3ddac544_bf74_11e9_8188_d05099a0e2b9level0_row8\" class=\"row_heading level0 row8\" >ambiental</th>\n                        <td id=\"T_3ddac544_bf74_11e9_8188_d05099a0e2b9row8_col0\" class=\"data row8 col0\" >1168</td>\n            </tr>\n            <tr>\n                        <th id=\"T_3ddac544_bf74_11e9_8188_d05099a0e2b9level0_row9\" class=\"row_heading level0 row9\" >internacional</th>\n                        <td id=\"T_3ddac544_bf74_11e9_8188_d05099a0e2b9row9_col0\" class=\"data row9 col0\" >974</td>\n            </tr>\n    </tbody></table>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 3
    }
   ],
   "source": [
    "# verificação das classes dos acórdãos\n",
    "\n",
    "classes = df[0]\n",
    "print(len(classes.value_counts()))\n",
    "classes.value_counts().to_frame().style.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[5 2 8 5 5 3 6 5 6 0]\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "1653762"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 4
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# convertendo classes\n",
    "encoder = LabelEncoder()\n",
    "classes = encoder.fit_transform(classes)\n",
    "\n",
    "print(classes[:10])\n",
    "len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "0    pretensa estadual deduz apel extrem esbarr ved...\n1    embarg insist mesm razo recurs apresent oposic...\n2    acorda ora recorr nao neg vigenc direit federa...\n3    segund jurisprudenc superior tribunal justic a...\n4    especial superior tribunal justic decid ser ne...\n5    inadmissivel especial interpost fundament art ...\n6    inadmissivel exigenc recolh reu prisa requisit...\n7    tribunal justic assent compreensa part det leg...\n8    dentr limit leg vez caracteriz reincidenc agra...\n9    ato administraca reconhec direit correca monet...\nName: 1, dtype: object\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# criação da lista de ementas\n",
    "\n",
    "ementas = df[1]\n",
    "print(ementas[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Word2Vec(vocab=101959, size=100, alpha=0.025)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# uso de word2vec para a extração de features\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "ementas_list = [word for word in ementas.iteritems()]\n",
    "ementas_list_list = []\n",
    "for doc in ementas_list:\n",
    "    ementas_list_list.append(doc[1].split())\n",
    "\n",
    "X = Word2Vec(ementas_list_list, min_count=2)\n",
    "X.wv.init_sims() # para inicializar model.vw.syn0norm, necessário para o cálculo da média da\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# funções para calcular a média vetorial das palavras de cada ementa\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "            mean.append(wv.vectors[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        print(\"cannot compute similarity with no input %s\", words)\n",
    "        # FIXME: remove these examples in pre-processing\n",
    "        return np.zeros(wv.vector_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, post) for post in text_list ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# função para tokenizar as palavras com frequência maior que 2\n",
    "import nltk\n",
    "\n",
    "\n",
    "def w2v_tokenize_text(text):    \n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='portuguese'):\n",
    "        for word in nltk.word_tokenize(sent, language='portuguese'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Like any other supervised machine learning problem, we need to divide data into \n",
    "# 20% test set and 80% training set\n",
    "\n",
    "import gensim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=0)\n",
    "\n",
    "test_tokenized = test.apply(lambda r: w2v_tokenize_text(r[1]), axis=1).values\n",
    "train_tokenized = train.apply(lambda r: w2v_tokenize_text(r[1]), axis=1).values\n",
    "\n",
    "X_train = word_averaging_list(X.wv,train_tokenized)\n",
    "X_test = word_averaging_list(X.wv,test_tokenized)\n",
    "\n",
    "y_train = train[0]\n",
    "y_test = test[0]\n",
    "\n",
    "#print(X_train)\n",
    "#print(X_test)\n",
    "print(\"train: {},{}\".format(len(X_train),len(y_train)))\n",
    "print(\"test: {},{}\".format(len(X_test),len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Like any other supervised machine learning problem, we need to divide data into \n",
    "# 20% test set and 80% training set\n",
    "\n",
    "import gensim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=0)\n",
    "\n",
    "test_tokenized = test.apply(lambda r: w2v_tokenize_text(r[1]), axis=1).values\n",
    "train_tokenized = train.apply(lambda r: w2v_tokenize_text(r[1]), axis=1).values\n",
    "\n",
    "X_train = word_averaging_list(X.wv,train_tokenized)\n",
    "X_test = word_averaging_list(X.wv,test_tokenized)\n",
    "\n",
    "y_train = train[0]\n",
    "y_test = test[0]\n",
    "\n",
    "print(X_train)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "\n",
    "# Modelos para treinar\n",
    "names = [\"K Nearest Neighbors\", \"Decision Tree\", \"Random Forest\", \"Logistic Regression\", \"SGD Classifier\",\n",
    "         \"Naive Bayes\", \"SVM Linear\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(n_estimators=1000, random_state=0, class_weight='balanced'),\n",
    "    LogisticRegression(solver='lbfgs', multi_class='multinomial'),\n",
    "    SGDClassifier(max_iter = 100, tol=1e-3),\n",
    "    SVC(kernel = 'linear')\n",
    "]\n",
    "\n",
    "models = zip(names, classifiers)\n",
    "\n",
    "for name, classifier in models:\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)*100\n",
    "    print(\"{} Accuracy: {}\".format(name, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Classificador por votos\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "names = [\"K Nearest Neighbors\", \"Decision Tree\", \"Random Forest\", \"Logistic Regression\", \"SGD Classifier\",\n",
    "         \"Naive Bayes\", \"SVM Linear\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(n_estimators=1000, random_state=0),\n",
    "    LogisticRegression(solver='lbfgs', multi_class='multinomial'),\n",
    "    SGDClassifier(max_iter = 100, tol=1e-3),\n",
    "    SVC(kernel = 'linear')\n",
    "]\n",
    "\n",
    "models = list(zip(names, classifiers))\n",
    "zipped_models_1 = models[:]\n",
    "zipped_models_2 = list(models)\n",
    "\n",
    "\n",
    "votingClassifier = VotingClassifier(estimators = zipped_models_2, voting = 'hard', n_jobs = -1)\n",
    "votingClassifier.fit(X_train, y_train)\n",
    "y_pred = votingClassifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)*100\n",
    "print(\"Voting Classifier: Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# relatório de classificação\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "#print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# usando undersampling\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "rus.fit(X_train, y_train)\n",
    "X_resampled, y_resampled = rus._fit_resample(X_train, y_train)\n",
    "print(\"train: {},{}\".format(len(X_resampled),len(y_resampled)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Fazendo a mesma análisa usando undersampling\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "\n",
    "# Modelos para treinar\n",
    "names = [\"K Nearest Neighbors\", \"Decision Tree\", \"Random Forest\", \"Logistic Regression\", \"SGD Classifier\",\n",
    "         \"Naive Bayes\", \"SVM Linear\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(n_estimators=1000, random_state=0, class_weight='balanced'),\n",
    "    LogisticRegression(solver='lbfgs', multi_class='multinomial'),\n",
    "    SGDClassifier(max_iter = 100, tol=1e-3),\n",
    "    SVC(kernel = 'linear')\n",
    "]\n",
    "\n",
    "models = zip(names, classifiers)\n",
    "\n",
    "for name, classifier in models:\n",
    "    classifier.fit(X_resampled, y_resampled)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)*100\n",
    "    print(\"{} Accuracy: {}\".format(name, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Classificador por votos\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "names = [\"K Nearest Neighbors\", \"Decision Tree\", \"Random Forest\", \"Logistic Regression\", \"SGD Classifier\",\n",
    "         \"Naive Bayes\", \"SVM Linear\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(n_estimators=1000, random_state=0, class_weight='balanced'),\n",
    "    LogisticRegression(solver='lbfgs', multi_class='multinomial'),\n",
    "    SGDClassifier(max_iter = 100, tol=1e-3),\n",
    "    SVC(kernel = 'linear')\n",
    "]\n",
    "\n",
    "models = list(zip(names, classifiers))\n",
    "zipped_models_1 = models[:]\n",
    "zipped_models_2 = list(models)\n",
    "\n",
    "\n",
    "votingClassifier = VotingClassifier(estimators = zipped_models_2, voting = 'hard', n_jobs = -1)\n",
    "votingClassifier.fit(X_resampled, y_resampled)\n",
    "y_pred = votingClassifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)*100\n",
    "print(\"Voting Classifier: Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# relatório de classificação\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "#print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-65cc7394",
   "language": "python",
   "display_name": "PyCharm (tcc)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}