{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import sklearn\n",
    "import pandas\n",
    "import numpy\n",
    "\n",
    "print('Python: {}'.format(sys.version))\n",
    "print('NLTK: {}'.format(nltk.__version__))\n",
    "print('Scikit-learn: {}'.format(sklearn.__version__))\n",
    "print('Pandas: {}'.format(pandas.__version__))\n",
    "print('Numpy: {}'.format(numpy.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 793 entries, 0 to 792\n",
      "Data columns (total 2 columns):\n",
      "0    793 non-null bool\n",
      "1    793 non-null object\n",
      "dtypes: bool(1), object(1)\n",
      "memory usage: 7.0+ KB\n",
      "None\n",
      "       0                                                  1\n",
      "0  False  pretensa estadual deduz apel extrem esbarr ved...\n",
      "1  False  embarg insist mesm razo recurs apresent oposic...\n",
      "2  False  acorda ora recorr nao neg vigenc direit federa...\n",
      "3  False  tribunal justic assent compreensa part det leg...\n",
      "4  False  segund jurisprudenc superior tribunal justic a...\n"
     ]
    }
   ],
   "source": [
    "# Carregando dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('data/dataset_tratado.txt', header=None, encoding='utf-8', sep='\\t')\n",
    "df.fillna('x', inplace=True)\n",
    "print(df.info())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    525\n",
      "True     268\n",
      "Name: 0, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# verificação das classes dos acórdãos\n",
    "\n",
    "binary = df[0]\n",
    "print(binary.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 1 1 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "793"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# convertendo classes\n",
    "encoder = LabelEncoder()\n",
    "binary = encoder.fit_transform(binary)\n",
    "\n",
    "print(binary[:10])\n",
    "len(binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    pretensa estadual deduz apel extrem esbarr ved...\n",
      "1    embarg insist mesm razo recurs apresent oposic...\n",
      "2    acorda ora recorr nao neg vigenc direit federa...\n",
      "3    tribunal justic assent compreensa part det leg...\n",
      "4    segund jurisprudenc superior tribunal justic a...\n",
      "5    sobrest recurs especial ate julgament recurs e...\n",
      "6    inadmissivel exigenc recolh reu prisa requisit...\n",
      "7    dentr limit leg vez caracteriz reincidenc agra...\n",
      "8    sobr prescrica aca repetica indebit tributari ...\n",
      "9    tratas agrav instrument interpost contr decisa...\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# criação da lista de ementas\n",
    "ementas = df[1]\n",
    "print(ementas[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=3183, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# uso de word2vec para a extração de features\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "ementas_list = [word for word in ementas.iteritems()]\n",
    "ementas_list_list = []\n",
    "for doc in ementas_list:\n",
    "    ementas_list_list.append(doc[1].split())\n",
    "\n",
    "X = Word2Vec(ementas_list_list, min_count=2)\n",
    "X.wv.init_sims() # para inicializar model.vw.syn0norm, necessário para o cálculo da média da\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funções para calcular a média vetorial das palavras de cada ementa\n",
    "\n",
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "            mean.append(wv.vectors[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        print(\"cannot compute similarity with no input %s\", words)\n",
    "        return np.zeros(wv.vector_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, post) for post in text_list ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# função para tokenizar as palavras com frequência maior que 2\n",
    "\n",
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='portuguese'):\n",
    "        for word in nltk.word_tokenize(sent, language='portuguese'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot compute similarity with no input %s []\n",
      "cannot compute similarity with no input %s []\n",
      "cannot compute similarity with no input %s []\n",
      "cannot compute similarity with no input %s []\n",
      "cannot compute similarity with no input %s []\n",
      "cannot compute similarity with no input %s []\n",
      "cannot compute similarity with no input %s []\n",
      "cannot compute similarity with no input %s []\n",
      "cannot compute similarity with no input %s []\n",
      "cannot compute similarity with no input %s []\n",
      "cannot compute similarity with no input %s []\n",
      "cannot compute similarity with no input %s []\n",
      "cannot compute similarity with no input %s []\n",
      "cannot compute similarity with no input %s []\n",
      "cannot compute similarity with no input %s []\n",
      "cannot compute similarity with no input %s []\n",
      "cannot compute similarity with no input %s []\n",
      "cannot compute similarity with no input %s []\n",
      "[[ 0.155413   -0.05996682 -0.13512392 ... -0.10904473 -0.01001627\n",
      "  -0.04182275]\n",
      " [ 0.1553461  -0.06026132 -0.13671583 ... -0.10850698 -0.01447462\n",
      "  -0.04481162]\n",
      " [ 0.15235671 -0.05916217 -0.13356668 ... -0.10722567 -0.0152533\n",
      "  -0.05115757]\n",
      " ...\n",
      " [ 0.15444115 -0.05884033 -0.13308664 ... -0.10986236 -0.01049536\n",
      "  -0.04263012]\n",
      " [ 0.15067384 -0.06624615 -0.14438143 ... -0.11345934 -0.01696481\n",
      "  -0.04972834]\n",
      " [ 0.15497355 -0.06068727 -0.13586596 ... -0.10928544 -0.0125477\n",
      "  -0.04353399]]\n",
      "[[ 0.15410866 -0.06299288 -0.14180668 ... -0.11154196 -0.01530378\n",
      "  -0.04689075]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.15536316 -0.05843928 -0.13845076 ... -0.10814518 -0.01644154\n",
      "  -0.04678126]\n",
      " ...\n",
      " [ 0.15688719 -0.05970938 -0.13742052 ... -0.10750055 -0.01439965\n",
      "  -0.04665949]\n",
      " [ 0.14665782 -0.06290177 -0.13584183 ... -0.11195672 -0.01818594\n",
      "  -0.04534222]\n",
      " [ 0.16047369 -0.06235959 -0.13202833 ... -0.10603005 -0.00607369\n",
      "  -0.04156844]]\n"
     ]
    }
   ],
   "source": [
    "# Like any other supervised machine learning problem, we need to divide data into \n",
    "# 20% test set and 80% training set\n",
    "\n",
    "import gensim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=0)\n",
    "\n",
    "test_tokenized = test.apply(lambda r: w2v_tokenize_text(r[1]), axis=1).values\n",
    "train_tokenized = train.apply(lambda r: w2v_tokenize_text(r[1]), axis=1).values\n",
    "\n",
    "X_train = word_averaging_list(X.wv,train_tokenized)\n",
    "X_test = word_averaging_list(X.wv,test_tokenized)\n",
    "\n",
    "y_train = train[0]\n",
    "y_test = test[0]\n",
    "\n",
    "print(X_train)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Nearest Neighbors Accuracy: 73.58490566037736\n",
      "Decision Tree Accuracy: 65.40880503144653\n",
      "Random Forest Accuracy: 76.10062893081762\n",
      "Logistic Regression Accuracy: 62.893081761006286\n",
      "SGD Classifier Accuracy: 62.893081761006286\n",
      "Naive Bayes Accuracy: 62.893081761006286\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "\n",
    "# Modelos para treinar\n",
    "names = [\"K Nearest Neighbors\", \"Decision Tree\", \"Random Forest\", \"Logistic Regression\", \"SGD Classifier\",\n",
    "         \"Naive Bayes\", \"SVM Linear\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(n_estimators=1000, random_state=0),\n",
    "    LogisticRegression(solver='lbfgs'),\n",
    "    SGDClassifier(max_iter = 100),\n",
    "    SVC(kernel = 'linear')\n",
    "]\n",
    "\n",
    "models = zip(names, classifiers)\n",
    "\n",
    "for name, classifier in models:\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)*100\n",
    "    print(\"{} Accuracy: {}\".format(name, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier: Accuracy: 66.6666666667\n"
     ]
    }
   ],
   "source": [
    "# Classificador por votos\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "names = [\"K Nearest Neighbors\", \"Decision Tree\", \"Random Forest\", \"Logistic Regression\", \"SGD Classifier\",\n",
    "         \"Naive Bayes\", \"SVM Linear\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(n_estimators=1000, random_state=0),\n",
    "    LogisticRegression(solver='lbfgs'),\n",
    "    SGDClassifier(max_iter = 100),\n",
    "    SVC(kernel = 'linear')\n",
    "]\n",
    "\n",
    "models = list(zip(names, classifiers))\n",
    "zipped_models_1 = models[:]\n",
    "zipped_models_2 = list(models)\n",
    "\n",
    "\n",
    "votingClassifier = VotingClassifier(estimators = zipped_models_2, voting = 'hard', n_jobs = -1)\n",
    "votingClassifier.fit(X_train, y_train)\n",
    "y_pred = votingClassifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)*100\n",
    "print(\"Voting Classifier: Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[93  7]\n",
      " [46 13]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.67      0.93      0.78       100\n",
      "        True       0.65      0.22      0.33        59\n",
      "\n",
      "   micro avg       0.67      0.67      0.67       159\n",
      "   macro avg       0.66      0.58      0.55       159\n",
      "weighted avg       0.66      0.67      0.61       159\n",
      "\n",
      "66.66666666666666\n"
     ]
    }
   ],
   "source": [
    "# relatório de classificação\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print((accuracy_score(y_test, y_pred)*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
