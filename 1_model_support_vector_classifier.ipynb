{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.7.3 (default, Jun 24 2019, 04:54:02) \n",
      "[GCC 9.1.0]\n",
      "NLTK: 3.4.1\n",
      "Scikit-learn: 0.21.2\n",
      "Pandas: 0.24.2\n",
      "Numpy: 1.16.4\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import sklearn\n",
    "import pandas\n",
    "import numpy\n",
    "\n",
    "print('Python: {}'.format(sys.version))\n",
    "print('NLTK: {}'.format(nltk.__version__))\n",
    "print('Scikit-learn: {}'.format(sklearn.__version__))\n",
    "print('Pandas: {}'.format(pandas.__version__))\n",
    "print('Numpy: {}'.format(numpy.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 793 entries, 0 to 792\n",
      "Data columns (total 2 columns):\n",
      "0    793 non-null bool\n",
      "1    793 non-null object\n",
      "dtypes: bool(1), object(1)\n",
      "memory usage: 7.0+ KB\n",
      "None\n",
      "       0                                                  1\n",
      "0  False  mand seguranc inexecuca contrat sanca afast re...\n",
      "1  False  respons civil acident aer decretol n dol event...\n",
      "2  False  processual civil intimaca art cpc irregular na...\n",
      "3  False  process civil honorari execuca legitim part su...\n",
      "4  False  administr servidor public artig lei n artig le...\n"
     ]
    }
   ],
   "source": [
    "# Carregando dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('data/dataset_tratado.txt', header=None, encoding='utf-8', sep='\\t')\n",
    "print(df.info())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    525\n",
      "True     268\n",
      "Name: 0, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# verificação das classes dos acórdãos\n",
    "classes = df[0]\n",
    "print(classes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# convertendo classes em binário\n",
    "encoder = LabelEncoder()\n",
    "binary = encoder.fit_transform(classes)\n",
    "\n",
    "print(binary[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    mand seguranc inexecuca contrat sanca afast re...\n",
      "1    respons civil acident aer decretol n dol event...\n",
      "2    processual civil intimaca art cpc irregular na...\n",
      "3    process civil honorari execuca legitim part su...\n",
      "4    administr servidor public artig lei n artig le...\n",
      "5    impost sobr servic iss arrend mercantil incide...\n",
      "6    petica receb hab corpus processual penal art c...\n",
      "7    penal process penal recurs especial art incis ...\n",
      "8    tribut sujeit lancament homologaca praz prescr...\n",
      "9    transport intermunicipal licitaca aca anulator...\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# criação da lista de ementas\n",
    "ementas = df[1]\n",
    "print(ementas[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# criação do BOW (bag of words)\n",
    "all_words = []\n",
    "\n",
    "for ementa in ementas:\n",
    "    words = word_tokenize(ementa)\n",
    "    for w in words:\n",
    "        all_words.append(w)\n",
    "        \n",
    "all_words = nltk.FreqDist(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de palavras: 5114\n",
      "Palavras mais comuns: [('nao', 1735), ('recurs', 1056), ('agrav', 1018), ('especial', 869), ('embarg', 847), ('art', 812), ('n', 790), ('lei', 779), ('regimental', 641), ('sumul', 608), ('civil', 545), ('tribunal', 481), ('stj', 454), ('ser', 435), ('declaraca', 392)]\n"
     ]
    }
   ],
   "source": [
    "# Exibição do número total de palavras e as 15 mais comuns\n",
    "print('Número de palavras: {}'.format(len(all_words)))\n",
    "print('Palavras mais comuns: {}'.format(all_words.most_common(15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso das 1500 palavras mais comuns como características\n",
    "word_features = list(all_words.keys())[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mand\n",
      "seguranc\n",
      "inexecuca\n",
      "contrat\n",
      "sanca\n",
      "afast\n",
      "reexam\n",
      "prov\n",
      "clausul\n",
      "contratu\n",
      "sumul\n",
      "stj\n",
      "pretensa\n",
      "estadual\n",
      "deduz\n",
      "apel\n",
      "extrem\n",
      "esbarr\n",
      "vedaco\n",
      "cont\n",
      "med\n",
      "necessari\n",
      "conjunt\n",
      "faticoprobatori\n",
      "aut\n",
      "bem\n",
      "deslind\n",
      "controvers\n",
      "total\n",
      "inviavel\n",
      "agrav\n",
      "improv\n"
     ]
    }
   ],
   "source": [
    "# Função determina quais das 1500 palavras características estão contidas nas ementas\n",
    "\n",
    "def find_features(ementa):\n",
    "    words = word_tokenize(ementa)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features[word] = (word in words)\n",
    "\n",
    "    return features\n",
    "\n",
    "# Por exemplo\n",
    "features = find_features(ementas[0])\n",
    "for key, value in features.items():\n",
    "    if value == True:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando à todas as ementas\n",
    "ementas_zip = list(zip(ementas, binary))\n",
    "zipped_list_1 = ementas_zip[:]\n",
    "zipped_list_2 = list(ementas_zip) \n",
    "\n",
    "# embaralhando lista\n",
    "seed = 1\n",
    "np.random.seed = seed\n",
    "np.random.shuffle(zipped_list_2)\n",
    "\n",
    "# chamando a função find_features para cada ementa\n",
    "featuresets = [(find_features(text), label) for (text, label) in ementas_zip]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separação dos conjuntos de característas em treinamento e teste usando sklean\n",
    "from sklearn import model_selection\n",
    "\n",
    "training, testing = model_selection.train_test_split(featuresets, test_size = 0.25, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "594\n",
      "199\n"
     ]
    }
   ],
   "source": [
    "print(len(training))\n",
    "print(len(testing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC Accuracy: 77.88944723618091\n"
     ]
    }
   ],
   "source": [
    "# Importação das bibliotecas para uso dos modelos\n",
    "\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SklearnClassifier(SVC(kernel = 'linear'))\n",
    "\n",
    "# treinando o modelo com os dados de treino\n",
    "model.train(training)\n",
    "\n",
    "# teste do dataset\n",
    "accuracy = nltk.classify.accuracy(model, testing)*100\n",
    "print(\"SVC Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Nearest Neighbors Accuracy: 49.246231155778894\n",
      "Decision Tree Accuracy: 80.40201005025126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 80.90452261306532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 75.87939698492463\n",
      "SGD Classifier Accuracy: 78.89447236180904\n",
      "Naive Bayes Accuracy: 71.85929648241206\n",
      "SVM Linear Accuracy: 77.88944723618091\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "# Modelos para treinar\n",
    "names = [\"K Nearest Neighbors\", \"Decision Tree\", \"Random Forest\", \"Logistic Regression\", \"SGD Classifier\",\n",
    "         \"Naive Bayes\", \"SVM Linear\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    LogisticRegression(),\n",
    "    SGDClassifier(max_iter = 100),\n",
    "    MultinomialNB(),\n",
    "    SVC(kernel = 'linear')\n",
    "]\n",
    "\n",
    "models = zip(names, classifiers)\n",
    "\n",
    "for name, classifier in models:\n",
    "    nltk_model = SklearnClassifier(classifier)\n",
    "    nltk_model.train(training)\n",
    "    accuracy = nltk.classify.accuracy(nltk_model, testing)*100\n",
    "    print(\"{} Accuracy: {}\".format(name, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier: Accuracy: 77.88944723618091\n"
     ]
    }
   ],
   "source": [
    "# Classificador por votos\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "names = [\"K Nearest Neighbors\", \"Decision Tree\", \"Random Forest\", \"Logistic Regression\", \"SGD Classifier\",\n",
    "         \"Naive Bayes\", \"SVM Linear\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    LogisticRegression(),\n",
    "    SGDClassifier(max_iter = 100),\n",
    "    MultinomialNB(),\n",
    "    SVC(kernel = 'linear')\n",
    "]\n",
    "\n",
    "models = list(zip(names, classifiers))\n",
    "zipped_models_1 = models[:]\n",
    "zipped_models_2 = list(models)\n",
    "\n",
    "\n",
    "nltk_ensemble = SklearnClassifier(VotingClassifier(estimators = zipped_models_2, voting = 'hard', n_jobs = -1))\n",
    "nltk_ensemble.train(training)\n",
    "accuracy = nltk.classify.accuracy(nltk_model, testing)*100\n",
    "print(\"Voting Classifier: Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criação de classe de label de predição para conjunto de teste\n",
    "txt_features_2, labels_2 = list(zip(*testing))\n",
    "txt_features_1, labels_1 = txt_features_2[:], labels_2[:]\n",
    "txt_features, labels = list(txt_features_2), list(labels_2)\n",
    "\n",
    "prediction = nltk_ensemble.classify_many(txt_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.82      0.83       136\n",
      "           1       0.64      0.70      0.67        63\n",
      "\n",
      "    accuracy                           0.78       199\n",
      "   macro avg       0.75      0.76      0.75       199\n",
      "weighted avg       0.79      0.78      0.78       199\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">previsto</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>deferido</th>\n",
       "      <th>indeferido</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">resultado</th>\n",
       "      <th>deferido</th>\n",
       "      <td>111</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indeferido</th>\n",
       "      <td>19</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     previsto           \n",
       "                     deferido indeferido\n",
       "resultado deferido        111         25\n",
       "          indeferido       19         44"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matriz de confusão e um relatório de classificação\n",
    "print(classification_report(labels, prediction))\n",
    "\n",
    "pd.DataFrame(\n",
    "    confusion_matrix(labels, prediction),\n",
    "    index = [['resultado', 'resultado'], ['deferido', 'indeferido']],\n",
    "    columns = [['previsto', 'previsto'], ['deferido', 'indeferido']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
